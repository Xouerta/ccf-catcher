import pandas as pd
import joblib
import zxcvbn
import numpy as np
import re
from lightgbm import LGBMClassifier, early_stopping, log_evaluation
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from skopt import BayesSearchCV
from feature import extract_text_features

# åŠ è½½åŸºç¡€æ•°æ®é›†
df = pd.read_csv("../data/password_dataset.csv").dropna(subset=["password"])

# åŠ è½½å¼ºå¯†ç æ•°æ®é›†
strong_passwords = []
try:
    with open("../data/rockyou.txt", "r", encoding="latin-1") as f:
        for line in f:
            pwd = line.strip()
            if 12 <= len(pwd) <= 64:
                strong_passwords.append(pwd)
except FileNotFoundError:
    print("âš ï¸ æœªæ‰¾åˆ° rockyou.txt æ–‡ä»¶ï¼Œä»…ä½¿ç”¨åŸºç¡€æ•°æ®é›†ã€‚")

# åˆ›å»º DataFrame å¹¶æ ‡è®°æ ‡ç­¾
strong_df = pd.DataFrame({
    "password": strong_passwords,
    "label": 1  # å¼ºå¯†ç æ ‡è®°ä¸º 1
})
df = pd.concat([df, strong_df], ignore_index=True)

# ç”Ÿæˆå¯†ç å¼ºåº¦æ ‡ç­¾
def generate_label(password):
    result = zxcvbn.zxcvbn(password)
    return 1 if result['score'] >= 3 else 0

print("â³ æ­£åœ¨ç”Ÿæˆå¯†ç å¼ºåº¦æ ‡ç­¾...")
df["label"] = df["password"].apply(generate_label)

# ç‰¹å¾æå–
print("ğŸ” æ­£åœ¨æå–å¯†ç ç‰¹å¾...")
X = extract_text_features(df["password"])
y = df["label"]

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    stratify=y,
    random_state=42
)

# è´å¶æ–¯ä¼˜åŒ–çš„å‚æ•°èŒƒå›´
param_space = {
    'num_leaves': (30, 150),  # å¶å­èŠ‚ç‚¹æ•°
    'learning_rate': (0.001, 0.2),  # å­¦ä¹ ç‡
    'min_child_samples': (20, 200),  # å¶å­æœ€å°æ ·æœ¬æ•°
    'reg_alpha': (0, 2),  # L1 æ­£åˆ™åŒ–
    'reg_lambda': (0, 2),  # L2 æ­£åˆ™åŒ–
    'colsample_bytree': (0.7, 1.0),  # æ¯æ¬¡è¿­ä»£çš„åˆ—é‡‡æ ·æ¯”ä¾‹
    'min_split_gain': (0.0, 0.5)  # æœ€å°åˆ†å‰²å¢ç›Š
}

# åˆå§‹åŒ– LightGBM æ¨¡å‹
model = LGBMClassifier(
    objective='binary',  # ç›®æ ‡æ˜¯äºŒåˆ†ç±»
    n_estimators=1000,  # æœ€å¤§è¿­ä»£æ¬¡æ•°
    class_weight='balanced',  # è‡ªåŠ¨è°ƒæ•´ç±»åˆ«æƒé‡
    random_state=42,
    n_jobs=-1,
    verbose=-1  # å…³é—­ LightGBM æ—¥å¿—è¾“å‡º
)

# è¿›è¡Œè´å¶æ–¯ä¼˜åŒ–æœç´¢
opt = BayesSearchCV(
    estimator=model,
    search_spaces=param_space,
    n_iter=50,  # æœç´¢ 50 ç»„å‚æ•°
    cv=StratifiedKFold(n_splits=5, shuffle=True),  # 5 æŠ˜äº¤å‰éªŒè¯
    scoring='roc_auc',  # è¯„ä»·æŒ‡æ ‡ï¼šAUC
    verbose=0,  # å…³é—­å†—ä½™æ—¥å¿—
    n_jobs=-1  # ä½¿ç”¨æ‰€æœ‰ CPU è¿›è¡Œè®¡ç®—
)

# å¼€å§‹è®­ç»ƒ
print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…...")
opt.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    eval_metric='auc',  # è¯„ä¼°æŒ‡æ ‡ï¼šAUC
    callbacks=[
        early_stopping(stopping_rounds=20, verbose=1),  # æ—©åœ
        log_evaluation(period=50)  # æ¯ 50 è½®è®°å½•ä¸€æ¬¡æ—¥å¿—
    ]
)

# è·å–æœ€ä½³æ¨¡å‹
best_model = opt.best_estimator_
print("\nğŸ¯ æœ€ä½³æ¨¡å‹å‚æ•°:")
for param, value in opt.best_params_.items():
    print(f"  - {param}: {value}")

# è¯„ä¼°æ¨¡å‹
y_pred = best_model.predict(X_test)
y_proba = best_model.predict_proba(X_test)[:, 1]

print(f"\nğŸ“Š æµ‹è¯•é›† AUC(æ›²çº¿ä¸‹é¢ç§¯): {roc_auc_score(y_test, y_proba):.4f}")
print("\nğŸ“ åˆ†ç±»æŠ¥å‘Š(precision, recall, f1-score):")
print(classification_report(y_test, y_pred))

print("\nğŸ”¢ æ··æ·†çŸ©é˜µï¼ˆé¢„æµ‹ vs çœŸå®ï¼‰:")
print(confusion_matrix(y_test, y_pred))

# ä¿å­˜æ¨¡å‹
joblib.dump(best_model, "../models/lgbm.pkl")
print("\nâœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³: ../models/lgbm.pkl")
